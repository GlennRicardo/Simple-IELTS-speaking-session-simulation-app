{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import numpy\n",
    "import tempfile\n",
    "import scipy.io.wavfile as wavfile\n",
    "import librosa\n",
    "import anthropic\n",
    "import json\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb1da24-d14e-45e8-b26b-67dff0ae5e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API key exists and begins sk-ant-a\n"
     ]
    }
   ],
   "source": [
    "# Load environment\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key     = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key  = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API key exists and begins {anthropic_api_key[:8]}\")\n",
    "\n",
    "else:\n",
    "    print(\"Anthropic API key not set\")\n",
    "\n",
    "# Setting instance\n",
    "openai    = OpenAI()\n",
    "claude    = anthropic.Anthropic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594d9540-7976-4b6d-add2-7ae88d84aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "examiner_system_message = \"You are an IELTS speaking session examiner that have one job to do conversation with examinee \\\n",
    "After the examinee greet you, you should respond their greetings and take them into conversation by choosing only one topic of your choice related to: \\\n",
    "1. Hometown & living place \\\n",
    "2. Education & Work \\\n",
    "3. Hobbies & Free Time \\\n",
    "4. Travel & Holidays \\\n",
    "5. Technology & Social Media \\\n",
    "You should respond in calm & friendly manner when engaging with conversation. Ask & Answer for no more than 1 sentence\\\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8077e45-4dd5-452a-be48-1278be497fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "appraiser_system_message = \"\"\"\n",
    "You are an IELTS speaking session appraiser that assesses the speaking skill of participants.\n",
    "For each user response in the conversation history:\n",
    "- Award +1 point if the response directly addresses the question asked by the assistant\n",
    "- Award -1 point if the response is irrelevant or unrelated to the question\n",
    "- Base your assessment purely on relevance, not on grammar, vocabulary, or fluency\n",
    "- The conversation consists of exactly 5 rounds of question-answer pairs\n",
    "- Calculate a final score between -5 and +5 based on these assessments\n",
    "- At the end, sum up the points\n",
    "\n",
    "Analyze only the content provided in the conversation history. Do not ask for additional information. Answer only with this json format\n",
    "[{'final_score': score}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113fc04f-c709-4553-84ca-fa0f61448e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_function = {\n",
    "    \"name\": \"get_category\",\n",
    "    \"description\": \"Assigns a category label based on a numeric score\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"score\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The numeric score to be categorized\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"score\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28457962-0e33-461d-8de9-738b1e10d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def appraiser(history):\n",
    "    message = claude.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=200,\n",
    "        temperature=1,\n",
    "        system=appraiser_system_message,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"This is how the conversation is going: {history}\"}\n",
    "        ],\n",
    "        tools=[category_function]\n",
    "    )\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54985588-34ef-4b9f-b58e-6d67df53ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "\n",
    "def play_audio(audio_segment):\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    temp_path = os.path.join(temp_dir, \"temp_audio.wav\")\n",
    "    try:\n",
    "        audio_segment.export(temp_path, format=\"wav\")\n",
    "        time.sleep(3) # Student Dominic found that this was needed. You could also try commenting out to see if not needed on your PC\n",
    "        subprocess.call([\n",
    "            \"ffplay\",\n",
    "            \"-nodisp\",\n",
    "            \"-autoexit\",\n",
    "            \"-hide_banner\",\n",
    "            temp_path\n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(temp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    " \n",
    "def talker(message):\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",  # Also, try replacing onyx with alloy\n",
    "        input=message\n",
    "    )\n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play_audio(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf89bf5-b48b-4c75-9929-1175547c5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for function call\n",
    "category = {5:\"excellent\", 4:\"very good\", 3:\"good\", 2:\"passed\", 1:\"practice more\"}\n",
    "\n",
    "def get_category(score):\n",
    "    return category.get(score,\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87be8e76-38ee-4df8-8d05-a18ac512b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle tool call function\n",
    "def handle_tool_call(message):\n",
    "    tool_use = message.content[0].input\n",
    "    score = tool_use[\"score\"]\n",
    "    category = get_category(score)\n",
    "    \n",
    "    return score, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a75e36c4-8867-474f-a3f0-ff6790ce56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def artist(score,category):\n",
    "    image_response = openai.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=f\"An image of  pop-art style certificate represents of IELTS Speaking Exercise consist of description about the score that users get {score} & and the achievement category {category} that users get\",\n",
    "            size=\"1024x1024\",\n",
    "            n=1,\n",
    "            response_format=\"b64_json\",\n",
    "        )\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dd00c5f-b708-43cc-a358-fe2b0fbac2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_message(history_data, counter_value):\n",
    "    \n",
    "    image = None\n",
    "    \n",
    "    if not history_data or history_data[-1][\"role\"] != \"user\":\n",
    "        return history_data, counter_value, image \n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": examiner_system_message}] + history_data\n",
    "    \n",
    "    try:\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "        reply = response.choices[0].message.content\n",
    "        \n",
    "        # the assistant's response to history\n",
    "        new_history = history_data + [{\"role\":\"assistant\", \"content\":reply}]\n",
    "        \n",
    "        new_counter = counter_value + 1\n",
    "        print(f\"Conversation turn: {new_counter}\")\n",
    "        \n",
    "        if new_counter == 4:\n",
    "            claude_response = appraiser(new_history)\n",
    "            print(claude_response)\n",
    "            if claude_response.content[0].type == \"tool_use\":\n",
    "                score, category = handle_tool_call(claude_response)\n",
    "                image = artist(score, category)\n",
    "            new_counter = 0\n",
    "            \n",
    "        talker(reply)\n",
    "        \n",
    "        return new_history, new_counter, image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error generating response:\", e)\n",
    "        return history_data, counter_value, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c9a370f-f3fa-4229-8fd5-8b71ca811aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_conversation():\n",
    "    return [], 0  # Clear history and reset counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8624061-b188-4a0a-aaeb-36850678709d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 834, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\responses.py\", line 359, in __call__\n",
      "    await self._handle_simple(send, send_header_only)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\responses.py\", line 388, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 162, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 544, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 834, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\responses.py\", line 359, in __call__\n",
      "    await self._handle_simple(send, send_header_only)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\responses.py\", line 388, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 162, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 544, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation turn: 1\n",
      "Conversation turn: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 834, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\responses.py\", line 359, in __call__\n",
      "    await self._handle_simple(send, send_header_only)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\responses.py\", line 388, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 162, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 544, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 834, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\responses.py\", line 359, in __call__\n",
      "    await self._handle_simple(send, send_header_only)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\responses.py\", line 388, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 162, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\glenn\\anaconda3\\envs\\llms\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 544, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation turn: 3\n"
     ]
    }
   ],
   "source": [
    "# Gradio Apps\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        image_output = gr.Image(height=500)\n",
    "    with gr.Row():\n",
    "        entry = gr.Microphone(label=\"Begin by greeting your IELTS Examiner\", type=\"filepath\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    # Maintain the conversation history as state\n",
    "    history = gr.State([])\n",
    "    counter = gr.State(0)\n",
    "    \n",
    "    def process_audio(audio_filepath, history_data):\n",
    "        if audio_filepath is None:\n",
    "            return history_data\n",
    "        \n",
    "        audio = open(audio_filepath, \"rb\")\n",
    "        try:\n",
    "            transcription = openai.audio.transcriptions.create(\n",
    "                model=\"gpt-4o-mini-transcribe\",\n",
    "                file=audio\n",
    "            )\n",
    "            user_message = transcription.text\n",
    "            \n",
    "            # Create a new history by appending the user message\n",
    "            new_history = history_data + [{\"role\":\"user\", \"content\":user_message}]\n",
    "            \n",
    "            return new_history\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error transcribing:\", e)\n",
    "            return history_data  # Return unchanged history on error\n",
    "\n",
    "\n",
    "    entry.change(\n",
    "            process_audio, \n",
    "            [entry, history], \n",
    "            [history]\n",
    "        ).then(\n",
    "            respond_to_message,\n",
    "            [history, counter],\n",
    "            [history, counter,image_output]\n",
    "        ).then(\n",
    "            lambda h: h,  # Pass through function to update UI\n",
    "            [history],\n",
    "            [chatbot]\n",
    "        )\n",
    "    \n",
    "    # Clear button resets both the UI and the state\n",
    "    clear.click(\n",
    "            clear_conversation, \n",
    "            [], \n",
    "            [history, counter], \n",
    "            queue=False\n",
    "        ).then(\n",
    "            lambda: None,  # This updates the UI\n",
    "            None, \n",
    "            chatbot, \n",
    "            queue=False\n",
    "        )\n",
    "\n",
    "ui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
